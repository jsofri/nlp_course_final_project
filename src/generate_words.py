import os
import json
import random
from argparse import ArgumentParser

import pyphen
import gensim
from gensim.downloader import load
from utils import Syllable, Word, DEFAULT_AUTOGENERATED_WORDS_FILE, DEFAULT_AUTOGENERATED_SYLLABLES_FILE, CACHE_DIR


MIN_SYLLABLES = 2
MAX_SYLLABLES = 8
SYLLABLES_PROBABILIES = [0.30, 0.30, 0.30, 0.04, 0.03, 0.02, 0.01]
NUM_SYLLABLES = [4, 3, MIN_SYLLABLES, 5, 6, 7, MAX_SYLLABLES]

MODEL_NAME = "glove-wiki-gigaword-200"
MODEL_PATH = os.path.join(CACHE_DIR, MODEL_NAME + ".model")


if os.path.exists(MODEL_PATH):
    print(f"Loading {MODEL_NAME} from local file")
    __model = gensim.models.KeyedVectors.load(MODEL_PATH)
else:
    print(f"Loading {MODEL_NAME}, this might take a while")
    __model = load(MODEL_NAME)
    __model.save(MODEL_PATH)
    print(f"Model saved locally at {MODEL_PATH}")

__vocab = [word for word in __model.index_to_key if word.isalpha()]  # remove none alphabetical words
__hyphenation = pyphen.Pyphen(lang='en')


def split_syllables_pyphen(word):
    hyphenated_word = __hyphenation.inserted(word)
    syllables = hyphenated_word.split('-')
    return syllables


def get_random_num_syllables():
    return random.choices(NUM_SYLLABLES, SYLLABLES_PROBABILIES)[0]


def generate_english_words(num_words: int, real_words: list[str]) -> list[Word]:
    print("Start generating English words")
    words = []
    chosen_words = set()
    while len(words) < num_words:
        english_word = random.choice(real_words)
        syllables = split_syllables_pyphen(english_word)
        if MIN_SYLLABLES <= len(syllables) <= MAX_SYLLABLES and english_word not in chosen_words:
            word = Word([Syllable(syllable, '') for syllable in syllables])
            words.append(word)
            chosen_words.add(english_word)
    return words


def generate_non_english_words(num_words: int, syllables: list[Syllable], real_words: set[str]) -> list[Word]:
    print("Start generating non-English words")
    conflicts = 0
    words = []
    while len(words) < num_words:
        tokens = []
        num_syllables = get_random_num_syllables()
        for _ in range(num_syllables):
            syllable = random.choice(syllables)
            tokens.append(syllable)
        word = Word(tokens)
        if word.word not in real_words:
            words.append(word)
        else:
            conflicts += 1
    print(f"total conflicts: {conflicts}")
    return words


def get_words(num_words: int, syllables: list[Syllable]) -> ord:
    if syllables:
        return generate_non_english_words(num_words, syllables, set(__vocab))
    else:
        return generate_english_words(num_words, __vocab)
            

def run(num_words: int, syllables_file: str, output_file: str):
    syllables = None
    if syllables_file:
        with open(syllables_file, 'r') as f:
            syllables_json = json.load(f)
        syllables = Syllable.json_to_list(syllables_json)

    words = get_words(num_words, syllables)
    try:
        as_json = Word.list_to_json(words)
    except Exception:
        # words are strings, not Word objects
        as_json = words

    with open(output_file, 'w') as f:
        json.dump(as_json, f, indent=4)

    if syllables_file is None:
        print(f"Samples {num_words} real words from {MODEL_NAME}")
    else:
        print(f"Generated {num_words} non-English words")
    print(f"Output file: {output_file}")


if __name__ == "__main__":
    parser = ArgumentParser(description="Generate a dataset of random words")
    parser.add_argument("--num_words", type=int,  default=1000,
                        help="Number of words to generate")
    parser.add_argument("--syllables_file", type=str,  default=DEFAULT_AUTOGENERATED_SYLLABLES_FILE,
                        help="Path to the syllables file used to make non-English words")
    parser.add_argument("--output_file", type=str,  default=DEFAULT_AUTOGENERATED_WORDS_FILE,
                        help="Output file name")
    args = parser.parse_args()

    run(args.num_words, args.syllables_file, args.output_file)
